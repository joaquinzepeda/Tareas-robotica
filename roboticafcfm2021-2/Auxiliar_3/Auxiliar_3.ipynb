{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auxiliar_3",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOEDjq9ZZqGsT3vMIp1c0j7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaimundoLorca/roboticafcfm2021-2/blob/main/Auxiliar_3/Auxiliar_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9TRB_NwHtP9"
      },
      "source": [
        "#Auxiliar 3: Feature Detection\n",
        "\n",
        "El Scale-Invariant Feature Transform (SIFT) es un algoritmo, desarrollado por D. Lowe en 2004, para la extracción de features de una imagen. En un sentido abstracto, las features de una imagen son patrones o regiones fácilmente detectables y trackeables, a partir de las cuales podemos identificar y reconocer toda de objetos presentes en la imagen. Así, en términos simples, el algoritmo SIFT detecta puntos de interés keypoints dentro de una imágen y luego, si estos son lo suficientemente distinguibles, genera un desciptor para cada keypoint que permita identificarlo o, más importante, compararlo en el futuro.\n",
        "\n",
        "https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/RaimundoLorca/roboticafcfm2021-2/main/bin/Ej_aux3.png\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lfJnla0JL9O"
      },
      "source": [
        "## OpenCV\n",
        "\n",
        "La Open Surce Computer Vision es una librería especializada en herramientas de visión computacional y en todo lo que respecta a visión artificial en general. De este modo provee de funcionalidades de todo tipo de complejidad, desde operaciones básicas de procesamiento de imágenes, hasta algoritmos de reconocimiento de objetos.\n",
        "\n",
        "Por supuesto, esta librería cuenta con su documentación correspondiente. En esta puede encontrar descripciones más detalladas de sus funcionalidades, así como también ejemplos, tutoriales y otros.\n",
        "\n",
        "https://docs.opencv.org/master/\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/roboticafcfm/master/auxiliar_01/bin/opencv_logo.png\" height=\"200\">\n",
        "\n",
        "Ahora, debido a conflictos de patentes la implementación de SIFT en OpenCV tuvo que ser movida a una librería complementaria opencv-contrib. De esta forma, para utilizarla en el taller debemos instalar la librería en el entorno de Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZskjN6DpogsN"
      },
      "source": [
        "!pip install opencv-python==4.5.3.56 opencv-contrib-python==4.5.3.56"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zitak4YsJYQ1"
      },
      "source": [
        "##Cargar Repositorio Github\n",
        "\n",
        "La imagen con que trabajaremos en este taller se encuentra en el repositorio del curso, de este modo lo primero que haremos será clonar este github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNAZdjoQHsHJ"
      },
      "source": [
        "!git clone https://github.com/RaimundoLorca/roboticafcfm2021-2.git\n",
        "%cd /content/roboticafcfm2021-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOVHorHDJsoF"
      },
      "source": [
        "##Cargar Imágenes de Referencia\n",
        "\n",
        "La imagen que contiene los objetos que queremos identificar se encuentra en el archivo Escena.png dentro del github del curso. No obstante, antes de procesar ésta debemos extraer las features de una serie de imagenes de referencia para así tener los keypoints y descriptors con los cuales podremos posteriormente comparar nuestra imagen de interés.\n",
        "\n",
        "\n",
        "A continuación cargaremos y exploraremos todas las imágenes contenidas en la carpeta `\"Auxiliar_3//bin\"`.\n",
        "\n",
        "Podemos ver con cuantas imágenes de referencia contamos mediante la función os.listdir, la cual retornará un listado de todos los archivos contenidos en la carpeta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF1uczuHDg9_"
      },
      "source": [
        "#Cargar librerias a usar\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "import os\n",
        "\n",
        "# Obtener lista con todos los archivos contenidos en la carpeta bin\n",
        "# para esto podemos usar la función os.listdir\n",
        "dir_path = \n",
        "img_list = \n",
        "\n",
        "# Print lista de imágenes\n",
        "print('Nombres imagenes de referencia:', img_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f4AX6iyGtct"
      },
      "source": [
        "Como se ve, en la carpeta `\"Auxiliar_3//bin\"` hay 3 imagenes en formato .png, en donde cada una corresponde a la imagen de referencia de los objetos que hay en la imagen `Escena.png`. Ahora, recorreremos esta lista para cargar las imágenes utilizando cv2.imread, generando una lista ref_imgs que contendrá los np.array con las imágenes de referencia.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulVbnPm0Hri9"
      },
      "source": [
        "# Inicializar lista que contendrá las imágenes np.array,\n",
        "# también una que contendrá el nombre de las imagenes\n",
        "ref_imgs, ref_names = [], []\n",
        "\n",
        "# Por cada imagen .png en img_list\n",
        "for filename in img_list:\n",
        "  # Obtener nombre utilizando os.path.splitext\n",
        "  img_name, _ = os.path.splitext(filename)\n",
        "\n",
        "  # Generar path del archivo con os.path.join\n",
        "  img_path = os.path.join(dir_path, filename)\n",
        "\n",
        "  # Load imagen con cv2.imread\n",
        "  img = \n",
        "\n",
        "  # Resize a tamaño de 350x250px con cv2.resize\n",
        "  img = \n",
        "\n",
        "  # Agregar a listas\n",
        "  ref_imgs.append(  )\n",
        "  ref_names.append(  )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Concatenar imágenes de referencia mediante np.hstack\n",
        "refs = np.hstack( ref_imgs )\n",
        "\n",
        "# Visualizar\n",
        "refs = cv2.cvtColor(refs, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#Plotear imagen con Matplotlib\n",
        "plot.figure(figsize=(15,15))\n",
        "plot.imshow(refs)\n",
        "plot.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMgVBcpvLbgv"
      },
      "source": [
        "Excelente, hemos logrado generar la lista ref_images con todas nuestras imágenes de referencia. Esta lista nos permitirá recorrer y procesar las imágenes de manera más cómoda y elegante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toU6PIccMrRf"
      },
      "source": [
        "##Extraer Keypoints de Referencia\n",
        "Ahora procesaremos esta serie de imágenes con SIFT para extraer sus KeyPoints y generar sus Descriptors correspondientes. La implementación de SIFT se encuentra en el módulo cv2.xfeatures2d el cual contiene distintos métodos de extracción de features y detección de keypoints como SIFT y SURF.\n",
        "\n",
        "https://docs.opencv.org/3.4.2/d0/d13/classcv_1_1Feature2D.html\n",
        "\n",
        "Para utilizar SIFT primero hay que inicializar el objeto mediante la función cv2.xfeatures2d.SIFT_create y luego utilizar la función cv2.xfeatures2d_SIFT.detectAndCompute sobre la imágen de interés para detectar los Keypoints y generar los Descriptors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLHanPuLMqsB"
      },
      "source": [
        "# Inicializar SIFT detector (cv2.xfeatures2d.SIFT_create())\n",
        "sift = \n",
        "\n",
        "# Incializar lista que contendrá los keypoints y descriptores\n",
        "# de las imágenes de referencia []\n",
        "ref_features = \n",
        "\n",
        "# Para cada una de las imágenes en ref_imgs\n",
        "for img in ref_imgs:\n",
        "\n",
        "  # Transformar a escala de grises (img de BGR a GRAY)\n",
        "  gray = \n",
        "\n",
        "  # Computar keypoint y descritores (sift.detectAndCompute(gray, mask=None))\n",
        "  kps, des = \n",
        "\n",
        "  # Agregar a lista (kps, des)\n",
        "  ref_features.append(  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMbqpY3loK82"
      },
      "source": [
        "Una vez generadas las features de una imagen, estas pueden ser visualizadas mediante la función cv2.drawKeypoints. Si a esta función se le entrega como parámetro flags el objeto cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, dibujará los KeyPoints utilizando la escala y orientación de estos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF3jMS6J1P0k"
      },
      "source": [
        "# Index en ref_imgs correspondiente a la imagen ref_2.png\n",
        "idx = 0\n",
        "\n",
        "# Extraer imagen y convertir a escala de grises (ref_imgs[idx] de BGR a GRAY)\n",
        "gray = \n",
        "\n",
        "# Extraer keypoints y descriptores ref_features[], idx\n",
        "kps, des = \n",
        "\n",
        "# Generar imagen donde se dibujarán los keypoints\n",
        "out = np.zeros_like(gray)\n",
        "\n",
        "# Dibujar keypoints cv2.drawKeypoints(gray, kps, out, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "out = \n",
        "\n",
        "#Plotear imagen con Matplotlib\n",
        "plot.figure(figsize=(10,10))\n",
        "plot.imshow(out)\n",
        "plot.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASu-Gdd13dM"
      },
      "source": [
        "##Feature Matching\n",
        "\n",
        "Ya que tenemos las features de nuestras imágenes de referencia en la lista ref_features, podemos pasar ahora a procesar nuestra imágen de interés `Escena.png.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyEfcadq2KAe"
      },
      "source": [
        "#Cargar imagen\n",
        "img_escena = cv2.imread(\"Auxiliar_3//Escena.png\")\n",
        "\n",
        "#Cambiar espacio de colores (BGR A RGB)\n",
        "img_rgb = cv2.cvtColor(img_escena, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#Plotear imagen con Matplotlib\n",
        "plot.figure(figsize=(10,10))\n",
        "plot.imshow(img_rgb)\n",
        "plot.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Edpl-hH2w4s"
      },
      "source": [
        "Del mismo modo en el que procesamos las imágenes de referencia, extraeremos los KeyPoints y Descriptors de esta imagen utilizando el mismo objeto cv2.xfeatures2d_SIFT anteriormente inicializado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLmInev72zSc"
      },
      "source": [
        "# Cambiar espacio de colores (RGB A GRAY)\n",
        "img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# Computar keypoint y descritores - sift.detectAndCompute(img_gray, mask=None)\n",
        "kps_escena, des_escena = \n",
        "\n",
        "# Inicializar imagen para visualizar\n",
        "out = np.zeros_like(img_gray)\n",
        "\n",
        "# Dibujar keypoints\n",
        "out = cv2.drawKeypoints(img_gray, kps_escena, out, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "#Plotear imagen con Matplotlib\n",
        "plot.figure(figsize=(10,10))\n",
        "plot.imshow(out)\n",
        "plot.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYQty_6E4dwn"
      },
      "source": [
        "Ahora que tenemos las features de nuestra imagen de interés debemos, debemos realizar un matching respecto a las features de las imágenes de referencia para analizar la cantidad de coincidencias y visualizar los matches. \n",
        "\n",
        "Para esto, dado que los descriptores consisten en vectores de (128, ) o bien, de 128 dimensiones, podemos utilizar como criterio la distancia euclidiana para evaluar que tan similares son dos Descriptors entre si o que tan cerca se encuentran en este espacio de 128 dimensiones. Así, realizaremos un matching de fuerza bruta mediante un cv2.BFMatcher, el cual a partir de un set de descriptores de query buscará los descriptores más cercanos en un set de reference para cada descriptor en el primer set.\n",
        "\n",
        "https://docs.opencv.org/3.4/d3/da1/classcv_1_1BFMatcher.html\n",
        "\n",
        "No obstante, ¿cómo podemos validar si los matches realizados por cv2.BFMatcher son buenas coincidencias?, pues siempre se podrá encontrar el descriptor más cercano a otro, incluso si estos no se encuentran a poca distancia.\n",
        "\n",
        "Para resolver esto D. Lowe propone utilizar un Ratio Test para validar los matches. Primero, en vez de buscar el descriptor más cercano a otro, se buscan los 2 descriptores más cercanos. Luego, bajo el supuesto de que los descriptores generados por SIFT son lo suficientemente distintivos, no debiese ser posible que hayan dos coincidencias o matches. Así, si existe un match válido, la distancia de la primera coincidencia debería ser considerablemente menor a la de la segunda. Dicho de otra forma, si ambas distancias de match son similares, entonces la verdad es que no se hayó ninguna coincidencia significativa.\n",
        "\n",
        "Para obtener los k descriptores más cercanos, podemos usar el método cv2.BFMatcher.knnMatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlyGvw6K4rtE"
      },
      "source": [
        "# Index en ref_imgs correspondiente a la imagen ref_i.png\n",
        "idx = 0\n",
        "\n",
        "# Extraer imagen de referencia\n",
        "img_ref = ref_imgs[idx]\n",
        "\n",
        "# Extraer keypoints y descriptores\n",
        "kps_ref, des_ref = ref_features[idx]\n",
        "\n",
        "# Inicializar BruteForce Matcher con criterios\n",
        "# estandares - cv2.BFMatcher()\n",
        "BruteForce = \n",
        "\n",
        "# Para cada decriptor de img_01.jpg, encontrar los 2 mejores matches\n",
        "# con respecto a los descriptores de las imágenes de referencia\n",
        "# BruteForce.knnMatch(des1, des2, k=2)\n",
        "matches = \n",
        "\n",
        "# Por cada par de matches (match_1, match_2) obtenido\n",
        "# para cada descriptor en des_ref\n",
        "good_matches = []\n",
        "for match_1, match_2 in matches:\n",
        "  # Aplicar ratio test para filtrar matches\n",
        "  ratio = \n",
        "  if match_1.distance < ratio*match_2.distance:\n",
        "    good_matches.append([match_1])\n",
        "\n",
        "# cv2.drawMatchesKnn expects list of lists as matches.\n",
        "outImg\t=\tcv2.drawMatchesKnn(img_ref, kps_ref, img_escena, kps_escena, good_matches,None, \n",
        "                         flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "#Plotear imagen con Matplotlib\n",
        "plot.figure(figsize=(10,10))\n",
        "plot.imshow(outImg)\n",
        "plot.axis(\"off\") "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}